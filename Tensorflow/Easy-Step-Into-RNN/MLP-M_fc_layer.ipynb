{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 全链接层\n",
    "全链接输入与输出矩阵格式为[BATCHSIZE, N]， 假设网络某一层输入为 x^l ，输出为 x^{l+1} ，那么输入与输出层间关系为：\n",
    "\n",
    "\\begin{matrix} h^l=x^l\\cdot W&(a)\\\\ u^l=h^l+b&(b)\\\\ x^{l+1}=f(u^l)&(c)\\\\ f(\\cdot)\\rightarrow {sigmoid}&(d)\\\\ sigmoid(x)=\\frac{1}{1+e^{-x}}&(e) \\end{matrix} (1.1)\n",
    "\n",
    "5.2 链式求导\n",
    "链式求导是目前为止整个深度学习的基础，有人将链式求导法则称之为反向传播。反向传播是从loss函数开始的。\n",
    "\n",
    "传播过程之中每一层均会计算两个内容：\n",
    "1. 本层可训练参数的导数，\n",
    "2. 本层向前传播误差（链式求导）。\n",
    "\n",
    "举个例子来说：\n",
    "\n",
    " \\begin{matrix} loss=(y-d)^2&(a)\\\\ y=f_1(a\\cdot f_2(b\\cdot f_3(c\\cdot x)))&(b)\\\\ error1=\\frac{\\partial loss}{\\partial y}=2 (y - d)&(c)\\\\ error2=\\frac{\\partial loss}{\\partial (a\\cdot f_2)}=error1\\cdot f_1'&(d)\\\\ \\frac{\\partial loss}{\\partial a}=error2\\cdot f_2&(e)\\\\ error3=\\frac{\\partial loss}{\\partial (f_2)}=error2\\cdot a&(f)\\\\ \\end{matrix}  (1.2)\n",
    "\n",
    "对于 a....f_2 这一层来说，有一个可训练参数a，那么反向传播需要计算可训练参数a的导数1.1-(e)，同时为了计算 f_3 之中的可训练参数，此层需要产生新的error3。对于 f_1(\\cdot) 这一层来说，由于没有可训练参数，因此仅产生反向传播误差error2。\n",
    "注意，1.2中将每一步计算，包括相乘、相加、通过函数均算为单独的计算层。因此全链接层包括：矩阵相乘（wx）-矩阵相加（wx+b）-函数计算（f(wx+b)）三个计算层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def _matmul(self, inputs, W, *args, **kw):\n",
    "        \"\"\"\n",
    "        正向传播\n",
    "        \"\"\"\n",
    "        return np.dot(inputs, W)\n",
    "    def _d_matmul(self, in_error, n_layer, layer_par):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        \"\"\"\n",
    "        W = self.value[n_layer]\n",
    "        inputs = self.outputs[n_layer]\n",
    "        self.d_value[n_layer] = np.dot(inputs.T, in_error)\n",
    "        error = np.dot(in_error, W.T)\n",
    "        return error\n",
    "    def matmul(self, filters, *args, **kw):\n",
    "        self.value.append(filters)\n",
    "        self.d_value.append(np.zeros_like(filters))\n",
    "        self.layer.append((self._matmul, None, self._d_matmul, None))\n",
    "        self.layer_name.append(\"matmul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add_Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bias_add(self, inputs, b, *args, **kw):\n",
    "        return inputs + b\n",
    "    def _d_bias_add(self, in_error, n_layer, layer_par):\n",
    "        self.d_value[n_layer] = np.sum(in_error, axis=0)\n",
    "        return in_error\n",
    "    def bias_add(self, bias, *args, **kw):\n",
    "        self.value.append(bias)\n",
    "        self.d_value.append(np.zeros_like(bias))\n",
    "        self.layer.append((self._bias_add, None, self._d_bias_add, None))\n",
    "        self.layer_name.append(\"bias_add\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sigmoid(self, X, *args, **kw):\n",
    "        return 1/(1+np.exp(-X))\n",
    "    def _d_sigmoid(self, in_error, n_layer, *args, **kw):\n",
    "        X = self.outputs[n_layer]\n",
    "        return in_error * np.exp(-X)/(1 + np.exp(-X)) ** 2\n",
    "    def sigmoid(self):\n",
    "        self.value.append([])\n",
    "        self.d_value.append([])\n",
    "        self.layer.append((self._sigmoid, None, self._d_sigmoid, None))\n",
    "        self.layer_name.append(\"sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _loss_square(self, Y, *args, **kw):\n",
    "        B = np.shape(Y)[0]\n",
    "        return np.square(self.outputs[-1] - Y)/B\n",
    "    def _d_loss_square(self, Y, *args, **kw):\n",
    "        B = np.shape(Y)[0]\n",
    "        return 2 * (self.outputs[-2] - Y)\n",
    "    def loss_square(self):\n",
    "        self.value.append([])\n",
    "        self.d_value.append([])\n",
    "        self.layer.append((self._loss_square, None, self._d_loss_square, None))    \n",
    "        self.layer_name.append(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train : \n",
    "\n",
    "w_new <- w_old + beta dot  dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, X):\n",
    "    self.outputs = []\n",
    "    self.outputs.append(X)\n",
    "    net = X\n",
    "    for idx, lay in enumerate(self.layer):\n",
    "        method, layer_par, _, _ = lay\n",
    "        net = method(net, self.value[idx], layer_par)\n",
    "        self.outputs.append(net)\n",
    "    return\n",
    "def backward(self, Y):\n",
    "    error = self.layer[-1][2](Y)\n",
    "    self.n_layer = len(self.value)\n",
    "    for itr in range(self.n_layer-2, -1, -1):\n",
    "        _, _, method, layer_par = self.layer[itr]\n",
    "        error = method(error, itr, layer_par)\n",
    "def apply_gradient(self, eta):\n",
    "    for idx, itr in enumerate(self.d_value):\n",
    "        if len(itr) == 0: continue\n",
    "        self.value[idx] -= itr * eta\n",
    "def fit(self, X, Y):\n",
    "    self.forward(X)\n",
    "    self.backward(Y)\n",
    "    self.apply_gradient(0.1)\n",
    "def predict(self, X):\n",
    "    self.forward(X)\n",
    "    return self.outputs[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN():\n",
    "    def __init__(self):\n",
    "        self.value = []\n",
    "        self.d_value = []\n",
    "        # 每一层输出\n",
    "        self.outputs = []\n",
    "        # 每一层所用函数\n",
    "        self.layer = []\n",
    "        # 层名\n",
    "        self.layer_name = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 初始化值\n",
    "iw1 = np.random.normal(0, 0.1, [28, 28])\n",
    "ib1 = np.zeros([28])\n",
    "iw2 = np.random.normal(0, 0.1, [28, 28])\n",
    "ib2 = np.zeros([28])\n",
    "iw3 = np.random.normal(0, 0.1, [28, 2])\n",
    "ib3 = np.zeros([2])\n",
    "##### 神经网络描述\n",
    "mtd = NN()\n",
    "mtd.matmul(iw1)\n",
    "mtd.bias_add(ib1)\n",
    "mtd.sigmoid()\n",
    "mtd.matmul(iw2)\n",
    "mtd.bias_add(ib2)\n",
    "mtd.sigmoid()\n",
    "mtd.matmul(iw3)\n",
    "mtd.bias_add(ib3)\n",
    "mtd.sigmoid()\n",
    "mtd.loss_square()\n",
    "###### 训练\n",
    "for itr in range(100):\n",
    "    ...\n",
    "    mtd.fit(inx, iny)\n",
    "\n",
    "\n",
    "8 运行结果\n",
    "输出模型：\n",
    "Layer 0: matmul\n",
    "Layer 1: bias_add\n",
    "Layer 2: sigmoid\n",
    "Layer 3: matmul\n",
    "Layer 4: bias_add\n",
    "Layer 5: sigmoid\n",
    "Layer 6: matmul\n",
    "Layer 7: bias_add\n",
    "Layer 8: sigmoid\n",
    "Layer 9: loss\n",
    "迭代100次后精度96%。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for https://zhuanlan.zhihu.com/p/37025766"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
