{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step into RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正向传播过程\n",
    "循环神经网络的输入是时序相关的，因此其输入与输入可以描述为 h_1 , ..... ,h_T 、 y_1, ..... ,y_T ，为了保持时序信息，最简单的RNN函数形式为：\n",
    "\n",
    "\\begin{matrix} h_t=f(x_t,h_{t-1})\\\\ \\rightarrow h_t=tanh(concat[x_t, h_{t-1}]\\cdot W+b)\\ \\rightarrow tanh(x_t\\cdot W1+h_{t-1}\\cdot W2+b) \\end{matrix} (1.1)\n",
    "\n",
    "其中x_t的形式为[BATCHSIZE, Features1]，h_t的形式为[BatchSize, Features2]。多层rnn网络可以在输出的基础上继续加入RNN函数：\n",
    "\n",
    "\\begin{matrix} h^l_t=f(x_t,h^l_{t-1})\\ h^{l+1}t=f(h^l_t, h^{l+1}{t-1}) \\end{matrix} (1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -Backward\n",
    "RNN函数反向传播过程与全链接网络类似：\n",
    "\n",
    "\\begin{matrix} e^l_{t-1}=\\frac{\\partial loss}{\\partial h^l_{t-1}}=\\frac{\\partial loss}{\\partial h^l_{t}}\\circ f'(x_t,h^l_{t-1})\\frac{\\partial(x_t\\cdot W1+h_{t-1}\\cdot W2+b)}{\\partial h_{t-1}}=e^l_t f' W2&(a)\\\\ \\Delta W1=\\sum_t (x_t)^T\\cdot (e_t^lf')&(b)\\\\ \\Delta W2=\\sum_t (h_{t-1})^T\\cdot (e_t^lf')&(c)\\\\ e^{l}{t}=\\frac{\\partial loss}{\\partial h^{l}{t}}=\\frac{\\partial loss}{\\partial h^{l+1}{t}}\\circ f'(h_t^l,h^{l+1}{t-1})\\frac{\\partial(h_t^l\\cdot W1+h_{t-1}\\cdot W2+b)}{\\partial h_{t}^l}=e^{l+1}_t f' W1&(d) \\end{matrix} (1.3)\n",
    "\n",
    "1.3-a称之为时间反向传播算法BPTT，1.3-c为层间传播。可训练参数为1.3-bc。实际上传统的RNN网络与全链接网络并无不同。只是添加了时间反向传播项。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class RNNCell():\n",
    "    def __init__(self, insize=12, outsize=6, type=\"BASIC\"):\n",
    "        self.outsize = outsize\n",
    "        self.insize = insize\n",
    "        self.w = np.random.uniform(-0.1, 0.1, [insize+outsize, outsize])\n",
    "        self.b = np.random.uniform(-0.1, 0.1, [outsize])\n",
    "        self.outputs = []\n",
    "        self.inputs = []\n",
    "        self.states = []\n",
    "    def tanh(self, x):\n",
    "        epx = np.exp(x)\n",
    "        enx = np.exp(-x)\n",
    "        return (epx-enx)/(epx+enx)\n",
    "    def __call__(self, x, s):\n",
    "        self.inputs.append(x)\n",
    "        self.inshape = np.shape(x)\n",
    "        self.states.append(s)\n",
    "        inx = np.concatenate([x, s], axis=1)\n",
    "        out = np.dot(inx, self.w) + self.b\n",
    "        self.outputs.append(out)\n",
    "        out = self.tanh(out)\n",
    "        return out, out\n",
    "    def assign(self, w, b):\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "    def zero_state(self, batch_size):\n",
    "        return np.zeros([batch_size, self.outsize])\n",
    "    def get_error(self, error):\n",
    "        self.error = error\n",
    "    def d_tanh(self, x):\n",
    "        e2x = np.exp(2 * x)\n",
    "        return 4 * e2x / (1 + e2x) ** 2\n",
    "    def backward(self):\n",
    "        self.back_error = [np.zeros(self.inshape) for itr in range(len(self.outputs))]\n",
    "        dw = np.zeros_like(self.w)\n",
    "        db = np.zeros([self.outsize])\n",
    "        w1 = self.w[:self.insize, :]\n",
    "        w2 = self.w[self.insize:, :]\n",
    "        for itrs in range(len(self.outputs)-1, -1, -1):\n",
    "            if len(self.error[itrs]) == 0:\n",
    "                continue\n",
    "            else:\n",
    "                err = self.error[itrs]\n",
    "            for itr in range(itrs, -1, -1):\n",
    "                h1 = self.outputs[itr]\n",
    "                h0 = self.states[itr]\n",
    "                x = self.inputs[itr]\n",
    "                d_fe = self.d_tanh(h1)\n",
    "                #print(\"es\", np.shape(err), itr)\n",
    "                err = d_fe * err\n",
    "                dw[:self.insize, :] += np.dot(x.T, err)\n",
    "                dw[self.insize:, :] += np.dot(h0.T, err)\n",
    "                db += np.sum(err, axis=0)\n",
    "                self.back_error[itr] += np.dot(err, w1.T)\n",
    "                #print(np.shape(self.back_error))\n",
    "                err = np.dot(err, w2.T)\n",
    "        self.dw = dw\n",
    "        self.db = db\n",
    "        return dw, db\n",
    "    def loss(self, y):\n",
    "        self.error = []\n",
    "        for itr in range(len(self.outputs)):\n",
    "            self.error.append([])\n",
    "        self.error[-1] = 2 * (self.tanh(self.outputs[-1]) - y)\n",
    "        self.error[-2] = 2 * (self.tanh(self.outputs[-2]) - y)\n",
    "\n",
    "\n",
    "class MultiRNNCells():\n",
    "    def __init__(self, rnn_cells):\n",
    "        print(rnn_cells)\n",
    "        self.cells = rnn_cells\n",
    "        self.cont = 0\n",
    "    def __call__(self, x, s):\n",
    "        state = []\n",
    "        out = x\n",
    "        for idx in range(len(self.cells)):\n",
    "            out, st = self.cells[idx](out, s[idx])\n",
    "            state.append(st)\n",
    "        self.cont += 1\n",
    "        return out, state\n",
    "    def tanh(self, x):\n",
    "        epx = np.exp(x)\n",
    "        enx = np.exp(-x)\n",
    "        return (epx-enx)/(epx+enx)\n",
    "    def loss(self, y):\n",
    "        self.error = []\n",
    "        for itr in range(self.cont):\n",
    "            self.error.append([])\n",
    "        self.error[-1] = 2 * (self.tanh(self.cells[-1].outputs[-1]) - y)\n",
    "        self.error[-2] = 2 * (self.tanh(self.cells[-1].outputs[-2]) - y)\n",
    "    def backward(self):\n",
    "        error = self.error\n",
    "        dws = []\n",
    "        for itr in range(len(self.cells)-1, -1, -1):\n",
    "            self.cells[itr].get_error(error)\n",
    "            self.cells[itr].backward()\n",
    "            error = self.cells[itr].back_error\n",
    "            dws.append(self.cells[itr].dw)\n",
    "            dws.append(self.cells[itr].db)\n",
    "        return tuple(dws)\n",
    "    def apply_gradient(self, eta=0.1):\n",
    "        dws = []\n",
    "        for itr in range(len(self.cells)-1, -1, -1):\n",
    "            self.cells[itr].w -= self.cells[itr].dw\n",
    "            self.cells[itr].b -= self.cells[itr].db\n",
    "            dws.append(self.cells[itr].dw)\n",
    "            dws.append(self.cells[itr].db)\n",
    "        return tuple(dws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "max_time = 10\n",
    "indata = tf.placeholder(dtype=tf.float64, shape=[batch_size, 10, 3])\n",
    "# 两层RNN网络\n",
    "cell = rnn.MultiRNNCell([rnn.BasicRNNCell(3) for itr in range(2)], state_is_tuple=True)\n",
    "state = cell.zero_state(batch_size, tf.float64)\n",
    "outputs = []\n",
    "states = []\n",
    "# 获取每一步输出，与状态\n",
    "for time_step in range(max_time):\n",
    "    (cell_output, state) = cell(indata[:, time_step, :], state)\n",
    "    outputs.append(cell_output)\n",
    "    states.append(state)\n",
    "y = tf.placeholder(tf.float64, shape=[batch_size, 3])\n",
    "# 定义loss函数\n",
    "loss = tf.square(outputs[-1]-y) + tf.square(outputs[-2]-y)\n",
    "opt = tf.train.GradientDescentOptimizer(1)\n",
    "# 获取可训练参数\n",
    "weights = tf.trainable_variables()\n",
    "# 计算梯度\n",
    "grad = opt.compute_gradients(loss, weights)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# 获取变量值与梯度\n",
    "w1, b1, w2, b2 = sess.run(weights)\n",
    "dw1, db1, dw2, db2 = sess.run(grad, feed_dict={indata:np.ones([batch_size, 10, 3]), y:np.ones([batch_size, 3])})\n",
    "dw1 = dw1[0]\n",
    "db1 = db1[0]\n",
    "dw2 = dw2[0]\n",
    "db2 = db2[0]\n",
    "rnn1 = RNNCell(3, 3)\n",
    "rnn1.assign(w1, b1)\n",
    "rnn2 = RNNCell(3, 3)\n",
    "rnn2.assign(w2, b2)\n",
    "state = []\n",
    "state.append(rnn1.zero_state(batch_size))\n",
    "state.append(rnn2.zero_state(batch_size))\n",
    "rnn = MultiRNNCells([rnn1, rnn2])\n",
    "indata = np.ones([batch_size, 10, 3])\n",
    "for time_step in range(max_time):\n",
    "    (cell_output, state) = rnn(indata[:, time_step, :], state)\n",
    "    print(cell_output)\n",
    "print(\"TF Gradients\", np.mean(dw1), np.mean(db1), np.mean(dw2), np.mean(db2))\n",
    "rnn.loss(np.ones([batch_size, 3]))\n",
    "dw2, db2, dw1, db1 = rnn.backward()\n",
    "print(\"NP Gradinets\", np.mean(dw1, np.mean(db1), np.mean(dw2), np.mean(db2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks for https://zhuanlan.zhihu.com/p/37025766"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
